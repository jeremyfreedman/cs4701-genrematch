{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN4JcSLjawnT"
      },
      "source": [
        "# GenreMatch\n",
        "Data cleaning and model generation  \n",
        "*Jeremy Freedman, Reza Madhavan, Kunal Sheth*"
      ],
      "id": "MN4JcSLjawnT"
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Archive.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdHUQFa3dgMx",
        "outputId": "c6c04ab2-854f-47e1-ea23-7eca8adc522b"
      },
      "id": "CdHUQFa3dgMx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Archive.zip\n",
            "   creating: utils/\n",
            "  inflating: utils/genius.py         \n",
            "   creating: utils/__pycache__/\n",
            "  inflating: utils/grab_lyrics.py    \n",
            "  inflating: utils/songlyrics.py     \n",
            "  inflating: utils/grab_genius.ipynb  \n",
            "  inflating: utils/azlyrics.py       \n",
            "  inflating: utils/__pycache__/songlyrics.cpython-37.pyc  \n",
            "   creating: data/\n",
            "  inflating: data/song_lyrics.csv    \n",
            "  inflating: data/rock_sl.csv        \n",
            "  inflating: data/lyrics.txt         \n",
            "  inflating: data/all_sl.csv         \n",
            "  inflating: data/country_sl.csv     \n",
            "  inflating: data/pop_sl.csv         \n",
            "  inflating: data/rap_sl.csv         \n",
            "  inflating: data/soul_sl.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy9GICSqdrpH",
        "outputId": "e0ac65c3-852f-40c1-c8d7-f9e3235717d1"
      },
      "id": "oy9GICSqdrpH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "azlyrics.py  grab_genius.ipynb\t__pycache__\n",
            "genius.py    grab_lyrics.py\tsonglyrics.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "scrolled": true,
        "id": "ZRQb4iM7awnV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import songlyrics as sl\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk import download as nltk_download\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import sklearn as sk\n",
        "from sklearn import tree\n",
        "import collections\n",
        "import gc"
      ],
      "id": "ZRQb4iM7awnV"
    },
    {
      "cell_type": "code",
      "source": [
        "a=pd.read_csv('data/song_lyrics.csv').dropna()\n",
        "new_df = a.copy()\n",
        "head = new_df.head(5)"
      ],
      "metadata": {
        "id": "S7jvCK5HjTtP"
      },
      "id": "S7jvCK5HjTtP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtGsK4NbawnW",
        "outputId": "14e2d8eb-19a4-4adb-d063-a382c0a85556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# download the NLTK stopwords list, if necessary\n",
        "nltk_download(['stopwords','vader_lexicon'])"
      ],
      "id": "mtGsK4NbawnW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZe4wN0lawnW",
        "outputId": "0bc467e2-feff-4662-f54e-0432d241e030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'dont', 'should', 'shouldve', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', 'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 'isn', 'isnt', 'ma', 'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', 'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt', 'im', 'ill', 'id', 'oh', 'cant', 'ive']\n"
          ]
        }
      ],
      "source": [
        "# we need to clean and update the NLTK stopwords for our data\n",
        "# we're stripping out punctuation entirely, which the stopwords are not equipped to handle\n",
        "# could optionally add music-centric stopwords ('oh', 'yeah', 'like', etc)\n",
        "\n",
        "temp_words = sw.words('english')\n",
        "stopwords = []\n",
        "additions = ['im', 'ill', 'id', 'oh', 'cant', 'ive']\n",
        "for w in temp_words:\n",
        "    stopwords.append(sl._clean(w))\n",
        "stopwords += additions\n",
        "print(stopwords)\n"
      ],
      "id": "rZe4wN0lawnW"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for index,row in new_df.iterrows():\n",
        "  if index % 100 == 0:\n",
        "    print(index)\n",
        "\n",
        "  song = row['Lyrics']\n",
        "  x=song.find('\\n')\n",
        "  song2 = sl._clean(song[x+1:])\n",
        "  new_song = ''\n",
        "  for i in range(len(song2)):\n",
        "    char = song2[i]\n",
        "\n",
        "    if char == '\\n':\n",
        "      new_song += ' '\n",
        "    elif char == '(' or char == ')':\n",
        "      new_song += ''\n",
        "    else:\n",
        "      new_song += char\n",
        "\n",
        "\n",
        "  new_song = new_song.encode('ascii','ignore').decode()\n",
        "  \n",
        "\n",
        "  song2 = new_song.split(' ')\n",
        "  song2 = list(filter(lambda x : x!='' and x not in stopwords, song2))\n",
        "  song2 = song2[:len(song2)-1]\n",
        "\n",
        "  new_df.at[index, 'Lyrics'] = song2\n"
      ],
      "metadata": {
        "id": "NXvZBjqH5iQa"
      },
      "id": "NXvZBjqH5iQa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordfreq = []\n",
        "totalfreq = {}\n",
        "for index,row in new_df.iterrows():\n",
        "  lyrics = row['Lyrics']\n",
        "  for word in lyrics:\n",
        "    totalfreq[word] = totalfreq.get(word,0) + 1\n",
        "  # x = dict(collections.Counter(lyrics))\n",
        "  # mapped = []\n",
        "  # for k in x.keys():\n",
        "  #   mapped.append((k,x[k]))\n",
        "  \n",
        "  # wordfreq.append(x)\n",
        "\n",
        "# new_df['WordFreq'] = wordfreq"
      ],
      "metadata": {
        "id": "mQAXliGOJ_hx"
      },
      "id": "mQAXliGOJ_hx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top1000 = dict(sorted(totalfreq.items(), key = lambda x : x[1], reverse = True))"
      ],
      "metadata": {
        "id": "RYF4IuvtRkpx"
      },
      "id": "RYF4IuvtRkpx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top1000keys = list(top1000.keys())[:1000]"
      ],
      "metadata": {
        "id": "PmW087LISP3B"
      },
      "id": "PmW087LISP3B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "new_df['Sentiment'] = 0.0\n",
        "\n",
        "for index,row in new_df.iterrows():\n",
        "  newlyrics = list(filter(lambda x : x in top1000keys, row['Lyrics']))\n",
        "\n",
        "  new_df.at[index, 'Sentiment'] = sia.polarity_scores(' '.join(newlyrics))['compound']\n",
        "  new_df.at[index, 'Lyrics'] = newlyrics\n"
      ],
      "metadata": {
        "id": "NSKULMcxy7ng"
      },
      "id": "NSKULMcxy7ng",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordfreq = []\n",
        "uniqwords = []\n",
        "for index,row in new_df.iterrows():\n",
        "  lyrics = row['Lyrics']\n",
        "  uniqwords.append(len(set(lyrics)))\n",
        "  x = {k : 0 for k in top1000keys}\n",
        "\n",
        "  for word in lyrics:\n",
        "    x[word] += 1\n",
        "\n",
        "\n",
        "  wordfreq.append(x)\n",
        "\n",
        "new_df['WordFreq'] = wordfreq\n",
        "new_df['UniqueWords'] = uniqwords\n",
        "new_df['FeatureVector'] = new_df.apply(lambda x : list(x['WordFreq'].values()) + + [x['Sentiment']] [x['UniqueWords']], axis = 1)\n"
      ],
      "metadata": {
        "id": "_fMC3pxO2Uql"
      },
      "id": "_fMC3pxO2Uql",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = {}\n",
        "for index,genre in enumerate(list(set(new_df['Genre']))):\n",
        "  d[genre] = index\n",
        "\n",
        "new_df['GenreInt'] = new_df.apply(lambda x : d[x['Genre']], axis = 1)\n",
        "\n",
        "d = {}\n",
        "for index,genre in enumerate(top1000keys):\n",
        "  d[genre] = index + 1\n",
        "\n",
        "new_df['LyricsInt'] = new_df.apply(lambda x : [d[word] for word in x['Lyrics']], axis = 1)\n"
      ],
      "metadata": {
        "id": "o1Uxt530D5xK"
      },
      "id": "o1Uxt530D5xK",
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a 80/20 train/test split of lyric words tagged by origin genre\n",
        "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(list(new_df['FeatureVector']), list(new_df['GenreInt']), test_size=0.25)\n",
        "\n",
        "print(f'Split size: {len(X_test) / (len(X_test) + len(X_train))}\\nTest set: {len(X_test)}\\nTrain set: {len(X_train)}')"
      ],
      "metadata": {
        "id": "MrdPGQ_F85Qy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bf496a6-be61-4b2d-ab7c-6ac5396cf67d"
      },
      "id": "MrdPGQ_F85Qy",
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split size: 0.2501226091221187\n",
            "Test set: 1020\n",
            "Train set: 3058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = tree.DecisionTreeClassifier()\n",
        "clf.fit(X_train,Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCqVqnRI_kje",
        "outputId": "a0bb9a2b-bc57-44b5-81a7-5e7279753dd4"
      },
      "id": "cCqVqnRI_kje",
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = clf.predict(X_test)\n",
        "np.sum(preds == Y_test)/len(preds)"
      ],
      "metadata": {
        "id": "4nUh3qoxJjhH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceec5fb9-0ba7-4ede-a9e5-7efda5c20f9a"
      },
      "id": "4nUh3qoxJjhH",
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33725490196078434"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "knn = sk.neighbors.KNeighborsClassifier(n_neighbors = 10)\n",
        "knn.fit(X_train, Y_train)\n",
        "\n",
        "preds = knn.predict(X_test)\n",
        "\n",
        "print(x,np.sum(preds == Y_test)/len(preds))\n",
        "\n"
      ],
      "metadata": {
        "id": "QSNwxP5nFbNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144bbad2-2b87-45fd-c405-5b810760ced0"
      },
      "id": "QSNwxP5nFbNJ",
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14 0.30196078431372547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "metadata": {
        "id": "lA25SCvoNIUy"
      },
      "id": "lA25SCvoNIUy",
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = GaussianNB()\n",
        "nb.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "K37sw_-rTms3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d05bd5e-31c8-4fe3-df0e-d8b9e288b794"
      },
      "id": "K37sw_-rTms3",
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB()"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = nb.predict(X_test)\n",
        "print(np.sum(preds == Y_test)/len(preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do-lKO88Tsqw",
        "outputId": "357ae5f3-f09c-4d35-e0cb-dc8bc4cdd247"
      },
      "id": "Do-lKO88Tsqw",
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3872549019607843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "# STOP"
      ],
      "metadata": {
        "id": "GWOk4XcpwP5h"
      },
      "id": "GWOk4XcpwP5h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CXrImqRawnX",
        "outputId": "9cc9c891-4090-4edc-a16e-1b45bdb625b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported 166 lines\n",
            "Identified 7 genres: {'soul_rb', 'alternative', 'rock', 'metal', 'rap_hiphop', 'pop', 'country'}\n",
            "Extracted 7 genres\n"
          ]
        }
      ],
      "source": [
        "# I'm opening the csv of lyrics from songlyrics.com, which is structured differently than the genius one.\n",
        "# might need to restructure this code (or the genius csv) if we want to use that too\n",
        "# the only difference is all the lyrics by each artist are combined into a single cell (as opposed to split by song)\n",
        "df = pd.read_csv('data/all_sl.csv')\n",
        "print(f'Imported {len(df)} lines')\n",
        "genres = set(df['Genre'])\n",
        "print(f'Identified {len(genres)} genres: {genres}')\n",
        "lyrics = defaultdict(str)\n",
        "for genre in genres:\n",
        "    for artist in list(df[df['Genre'] == genre]['Lyrics']): # grab each row (artist) and select the lyrics cell\n",
        "        lyrics[genre] += artist # combine each batch of lyrics into the respective genre in the dictionary\n",
        "print(f'Extracted {len(lyrics)} genres')"
      ],
      "id": "5CXrImqRawnX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNK8qUKlawnY",
        "outputId": "5f0af8aa-a661-4f0b-ffd8-0b97868483b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[pop] Identified 7269 unique words\n",
            "[rap_hiphop] Identified 15104 unique words\n",
            "[rock] Identified 6416 unique words\n",
            "[metal] Identified 8963 unique words\n",
            "[alternative] Identified 5750 unique words\n",
            "[country] Identified 6533 unique words\n",
            "[soul_rb] Identified 4626 unique words\n"
          ]
        }
      ],
      "source": [
        "frequency_tables = {}\n",
        "words_uniq = {}\n",
        "lines_uniq = {}\n",
        "for (genre,lyric) in lyrics.items():\n",
        "    frequency_tables[genre] = sl.words_freq(lyric, stopwords)\n",
        "    words_uniq[genre] = sl.words(lyric, stopwords)\n",
        "    print(f'[{genre}] Identified {len(words_uniq[genre])} unique words')"
      ],
      "id": "MNK8qUKlawnY"
    },
    {
      "cell_type": "code",
      "source": [
        "len(lyrics['alternative'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aCzIv-Ai5zU",
        "outputId": "b2d9b473-1d4d-4361-9620-b9d77950599d"
      },
      "id": "2aCzIv-Ai5zU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "703734"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyxLZ9lKawnZ",
        "outputId": "e4f4939e-2648-4d8e-b07a-69844249333f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### pop ###\n",
            "[('love', 2008), ('know', 1645), ('like', 1463), ('baby', 1121), ('got', 1087), ('yeah', 1053), ('go', 1007), ('get', 967), ('one', 868), ('cause', 867), ('let', 806), ('girl', 796), ('say', 727), ('make', 723), ('la', 709), ('want', 688), ('wanna', 667), ('time', 656), ('never', 643), ('need', 611), ('take', 606), ('right', 605), ('way', 580), ('heart', 573), ('see', 566)]\n",
            "### rap_hiphop ###\n",
            "[('like', 2874), ('got', 2079), ('get', 1991), ('nigga', 1790), ('know', 1750), ('aint', 1316), ('yeah', 1286), ('niggas', 1236), ('bitch', 1206), ('shit', 1184), ('fuck', 1143), ('love', 943), ('back', 891), ('see', 860), ('money', 799), ('go', 784), ('cause', 770), ('baby', 708), ('thats', 705), ('want', 702), ('man', 674), ('make', 669), ('em', 668), ('say', 660), ('right', 591)]\n",
            "### rock ###\n",
            "[('love', 1159), ('yeah', 869), ('know', 793), ('like', 679), ('got', 659), ('one', 628), ('away', 580), ('go', 551), ('get', 525), ('never', 525), ('time', 524), ('want', 520), ('come', 440), ('way', 432), ('well', 430), ('say', 423), ('baby', 422), ('take', 394), ('gonna', 347), ('see', 346), ('let', 333), ('give', 323), ('back', 309), ('life', 306), ('wanna', 302)]\n",
            "### metal ###\n",
            "[('one', 573), ('like', 565), ('never', 543), ('know', 507), ('time', 504), ('love', 494), ('see', 492), ('get', 473), ('away', 452), ('life', 449), ('take', 439), ('yeah', 434), ('let', 380), ('way', 368), ('die', 348), ('got', 348), ('feel', 343), ('go', 341), ('world', 337), ('eyes', 321), ('come', 316), ('inside', 303), ('say', 279), ('man', 274), ('want', 272)]\n",
            "### alternative ###\n",
            "[('know', 838), ('love', 678), ('time', 652), ('like', 599), ('go', 596), ('got', 543), ('one', 525), ('never', 506), ('take', 457), ('back', 447), ('away', 445), ('say', 440), ('come', 432), ('get', 418), ('want', 417), ('right', 405), ('see', 401), ('let', 376), ('yeah', 369), ('way', 369), ('cause', 368), ('na', 351), ('feel', 342), ('well', 325), ('make', 314)]\n",
            "### country ###\n",
            "[('like', 1033), ('love', 777), ('know', 714), ('got', 642), ('baby', 539), ('one', 524), ('little', 518), ('get', 516), ('yeah', 490), ('go', 488), ('time', 465), ('back', 452), ('aint', 434), ('never', 427), ('see', 418), ('gonna', 413), ('take', 396), ('well', 383), ('could', 376), ('night', 376), ('way', 367), ('cause', 365), ('man', 364), ('old', 354), ('away', 352)]\n",
            "### soul_rb ###\n",
            "[('love', 1492), ('baby', 1220), ('know', 971), ('yeah', 762), ('like', 627), ('got', 547), ('want', 477), ('come', 457), ('time', 439), ('one', 435), ('wanna', 435), ('go', 424), ('let', 395), ('make', 364), ('get', 350), ('say', 346), ('right', 310), ('way', 309), ('see', 307), ('girl', 306), ('cause', 300), ('day', 296), ('aint', 274), ('never', 273), ('good', 264)]\n"
          ]
        }
      ],
      "source": [
        "# as an example, print the top 25 most common terms from each genre!\n",
        "for genre in frequency_tables:\n",
        "    print(f'### {genre} ###\\n{sorted(frequency_tables[genre].items(), key=lambda x: x[1], reverse=True)[:25]}')"
      ],
      "id": "kyxLZ9lKawnZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "Ml5XQ3uSawnZ",
        "outputId": "4e8e4e75-23f3-436e-807a-7501608322f7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5cd872ee20da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgenre\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_uniq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenre\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'words_uniq' is not defined"
          ]
        }
      ],
      "source": [
        "# make a 80/20 train/test split of lyric words tagged by origin genre\n",
        "X = []\n",
        "Y = []\n",
        "for (genre,words) in words_uniq.items():\n",
        "    X += words\n",
        "    Y += [genre] * len(words)\n",
        "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X, Y, test_size=0.2)\n",
        "print(f'Split size: {len(X_test) / (len(X_test) + len(X_train))}\\nTest set: {len(X_test)}\\nTrain set: {len(X_train)}')\n"
      ],
      "id": "Ml5XQ3uSawnZ"
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "filtered.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}