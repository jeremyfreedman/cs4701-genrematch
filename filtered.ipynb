{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "MN4JcSLjawnT",
      "metadata": {
        "id": "MN4JcSLjawnT"
      },
      "source": [
        "# GenreMatch\n",
        "Data cleaning and model generation  \n",
        "*Jeremy Freedman, Reza Madhavan, Kunal Sheth*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "CdHUQFa3dgMx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdHUQFa3dgMx",
        "outputId": "a0254c9b-217b-45f7-bf43-3a89aa01dee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open Archive.zip, Archive.zip.zip or Archive.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip Archive.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "oy9GICSqdrpH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy9GICSqdrpH",
        "outputId": "14e938d0-d975-4709-ce9c-10562cc138cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__pycache__  genius.py\t\tgrab_lyrics.py\n",
            "azlyrics.py  grab_genius.ipynb\tsonglyrics.py\n"
          ]
        }
      ],
      "source": [
        "!ls utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "ZRQb4iM7awnV",
      "metadata": {
        "id": "ZRQb4iM7awnV",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import utils.songlyrics as sl\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk import download as nltk_download\n",
        "import sklearn as sk\n",
        "from sklearn import tree\n",
        "import collections\n",
        "import gc\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "S7jvCK5HjTtP",
      "metadata": {
        "id": "S7jvCK5HjTtP"
      },
      "outputs": [],
      "source": [
        "a=pd.read_csv('data/song_lyrics.csv').dropna()\n",
        "new_df = a.copy()\n",
        "head = new_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "mtGsK4NbawnW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtGsK4NbawnW",
        "outputId": "528a5e7e-cda7-4e1f-d316-6a5cbcf6c56b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/jerem/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# download the NLTK stopwords list, if necessary\n",
        "nltk_download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "rZe4wN0lawnW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZe4wN0lawnW",
        "outputId": "f31dbdc4-16f8-4b2b-9044-87b79d9a5eac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'dont', 'should', 'shouldve', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', 'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 'isn', 'isnt', 'ma', 'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', 'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt', 'im', 'ill', 'id', 'oh', 'cant', 'ive']\n"
          ]
        }
      ],
      "source": [
        "# we need to clean and update the NLTK stopwords for our data\n",
        "# we're stripping out punctuation entirely, which the stopwords are not equipped to handle\n",
        "# could optionally add music-centric stopwords ('oh', 'yeah', 'like', etc)\n",
        "\n",
        "temp_words = sw.words('english')\n",
        "stopwords = []\n",
        "additions = ['im', 'ill', 'id', 'oh', 'cant', 'ive']\n",
        "for w in temp_words:\n",
        "    stopwords.append(sl._clean(w))\n",
        "stopwords += additions\n",
        "print(stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "NXvZBjqH5iQa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "NXvZBjqH5iQa",
        "outputId": "faa8aa2e-281e-417f-bb18-0f52d1fd0d35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n"
          ]
        }
      ],
      "source": [
        "for index,row in new_df.iterrows():\n",
        "  if index % 100 == 0:\n",
        "    print(index)\n",
        "\n",
        "  song = row['Lyrics']\n",
        "  x=song.find('\\n')\n",
        "  song2 = sl._clean(song[x+1:])\n",
        "  new_song = ''\n",
        "  for i in range(len(song2)):\n",
        "    char = song2[i]\n",
        "\n",
        "    if char == '\\n':\n",
        "      new_song += ' '\n",
        "    elif char == '(' or char == ')':\n",
        "      new_song += ''\n",
        "    else:\n",
        "      new_song += char\n",
        "\n",
        "\n",
        "    new_song = new_song.encode('ascii','ignore').decode()\n",
        "\n",
        "  song2 = new_song.split(' ')\n",
        "  song2 = list(filter(lambda x : x!='' and x not in stopwords, song2))\n",
        "  song2 = song2[:len(song2)-1]\n",
        "\n",
        "  new_df.at[index, 'Lyrics'] = song2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "mQAXliGOJ_hx",
      "metadata": {
        "id": "mQAXliGOJ_hx"
      },
      "outputs": [],
      "source": [
        "wordfreq = []\n",
        "word_appearance_cnt = defaultdict(int) # how many songs contain word W? used for IDF\n",
        "for index,row in new_df.iterrows():\n",
        "  lyrics = row['Lyrics']\n",
        "  x = dict(collections.Counter(lyrics))\n",
        "  for w in x:\n",
        "    word_appearance_cnt[w] += 1\n",
        "  # mapped = []\n",
        "  # for k in x.keys():\n",
        "  #   mapped.append((k,x[k]))\n",
        "  wordfreq.append(x)\n",
        "\n",
        "new_df['WordFreq'] = wordfreq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "5t7YnPNfK6rg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "5t7YnPNfK6rg",
        "outputId": "1303df16-e040-45ed-ff4d-061d901a3181"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Artist</th>\n",
              "      <th>Song</th>\n",
              "      <th>Lyrics</th>\n",
              "      <th>Genre</th>\n",
              "      <th>WordFreq</th>\n",
              "      <th>UniqueWords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>One Direction</td>\n",
              "      <td>Perfect</td>\n",
              "      <td>[might, never, knight, shinin, armor, might, n...</td>\n",
              "      <td>pop</td>\n",
              "      <td>{'might': 4, 'never': 4, 'knight': 1, 'shinin'...</td>\n",
              "      <td>71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>One Direction</td>\n",
              "      <td>Story of My Life</td>\n",
              "      <td>[written, walls, stories, explain, leave, hear...</td>\n",
              "      <td>pop</td>\n",
              "      <td>{'written': 3, 'walls': 2, 'stories': 1, 'expl...</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>One Direction</td>\n",
              "      <td>Drag Me Down</td>\n",
              "      <td>[got, fire, heart, scared, dark, never, seen, ...</td>\n",
              "      <td>pop</td>\n",
              "      <td>{'got': 4, 'fire': 2, 'heart': 2, 'scared': 2,...</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>One Direction</td>\n",
              "      <td>What Makes You Beautiful</td>\n",
              "      <td>[insecure, know, turning, heads, walk, door, n...</td>\n",
              "      <td>pop</td>\n",
              "      <td>{'insecure': 1, 'know': 18, 'turning': 1, 'hea...</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>One Direction</td>\n",
              "      <td>Night Changes</td>\n",
              "      <td>[going, tonight, changes, something, red, moth...</td>\n",
              "      <td>pop</td>\n",
              "      <td>{'going': 2, 'tonight': 3, 'changes': 8, 'some...</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4074</th>\n",
              "      <td>4074</td>\n",
              "      <td>Def Leppard</td>\n",
              "      <td>Action! Not Words</td>\n",
              "      <td>[sick, tired, damn, tv, gonna, make, movie, wa...</td>\n",
              "      <td>metal</td>\n",
              "      <td>{'sick': 1, 'tired': 1, 'damn': 1, 'tv': 1, 'g...</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4075</th>\n",
              "      <td>4075</td>\n",
              "      <td>Def Leppard</td>\n",
              "      <td>Rock Rock (Till You Drop)</td>\n",
              "      <td>[ready, get, set, tear, place, apart, need, ti...</td>\n",
              "      <td>metal</td>\n",
              "      <td>{'ready': 1, 'get': 2, 'set': 1, 'tear': 1, 'p...</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4076</th>\n",
              "      <td>4076</td>\n",
              "      <td>Def Leppard</td>\n",
              "      <td>Slang</td>\n",
              "      <td>[sitting, dark, getting, taken, said, somethin...</td>\n",
              "      <td>metal</td>\n",
              "      <td>{'sitting': 1, 'dark': 1, 'getting': 1, 'taken...</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4077</th>\n",
              "      <td>4077</td>\n",
              "      <td>Def Leppard</td>\n",
              "      <td>Die Hard the Hunter</td>\n",
              "      <td>[lets, welcome, home, soldier, boy, far, away,...</td>\n",
              "      <td>metal</td>\n",
              "      <td>{'lets': 2, 'welcome': 2, 'home': 2, 'soldier'...</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4078</th>\n",
              "      <td>4078</td>\n",
              "      <td>Def Leppard</td>\n",
              "      <td>High ‘n’ Dry (Saturday Night)</td>\n",
              "      <td>[saturday, feel, right, drinking, day, yes, go...</td>\n",
              "      <td>metal</td>\n",
              "      <td>{'saturday': 17, 'feel': 3, 'right': 1, 'drink...</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4078 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0         Artist                           Song  \\\n",
              "0              0  One Direction                        Perfect   \n",
              "1              1  One Direction               Story of My Life   \n",
              "2              2  One Direction                   Drag Me Down   \n",
              "3              3  One Direction       What Makes You Beautiful   \n",
              "4              4  One Direction                  Night Changes   \n",
              "...          ...            ...                            ...   \n",
              "4074        4074    Def Leppard              Action! Not Words   \n",
              "4075        4075    Def Leppard      Rock Rock (Till You Drop)   \n",
              "4076        4076    Def Leppard                          Slang   \n",
              "4077        4077    Def Leppard            Die Hard the Hunter   \n",
              "4078        4078    Def Leppard  High ‘n’ Dry (Saturday Night)   \n",
              "\n",
              "                                                 Lyrics  Genre  \\\n",
              "0     [might, never, knight, shinin, armor, might, n...    pop   \n",
              "1     [written, walls, stories, explain, leave, hear...    pop   \n",
              "2     [got, fire, heart, scared, dark, never, seen, ...    pop   \n",
              "3     [insecure, know, turning, heads, walk, door, n...    pop   \n",
              "4     [going, tonight, changes, something, red, moth...    pop   \n",
              "...                                                 ...    ...   \n",
              "4074  [sick, tired, damn, tv, gonna, make, movie, wa...  metal   \n",
              "4075  [ready, get, set, tear, place, apart, need, ti...  metal   \n",
              "4076  [sitting, dark, getting, taken, said, somethin...  metal   \n",
              "4077  [lets, welcome, home, soldier, boy, far, away,...  metal   \n",
              "4078  [saturday, feel, right, drinking, day, yes, go...  metal   \n",
              "\n",
              "                                               WordFreq  UniqueWords  \n",
              "0     {'might': 4, 'never': 4, 'knight': 1, 'shinin'...           71  \n",
              "1     {'written': 3, 'walls': 2, 'stories': 1, 'expl...           68  \n",
              "2     {'got': 4, 'fire': 2, 'heart': 2, 'scared': 2,...           37  \n",
              "3     {'insecure': 1, 'know': 18, 'turning': 1, 'hea...           54  \n",
              "4     {'going': 2, 'tonight': 3, 'changes': 8, 'some...           65  \n",
              "...                                                 ...          ...  \n",
              "4074  {'sick': 1, 'tired': 1, 'damn': 1, 'tv': 1, 'g...           61  \n",
              "4075  {'ready': 1, 'get': 2, 'set': 1, 'tear': 1, 'p...           74  \n",
              "4076  {'sitting': 1, 'dark': 1, 'getting': 1, 'taken...           41  \n",
              "4077  {'lets': 2, 'welcome': 2, 'home': 2, 'soldier'...           65  \n",
              "4078  {'saturday': 17, 'feel': 3, 'right': 1, 'drink...           45  \n",
              "\n",
              "[4078 rows x 7 columns]"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_df['UniqueWords']= new_df.apply(lambda row : len(row['WordFreq']), axis = 1)\n",
        "new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "AhtEtNbEwmnH",
      "metadata": {
        "id": "AhtEtNbEwmnH"
      },
      "outputs": [],
      "source": [
        "uniq_words = set()\n",
        "for x in new_df['WordFreq']:\n",
        "  uniq_words = uniq_words.union(x.keys())\n",
        "uniq_wordict = {k : 0 for k in uniq_words}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "7-z_Fgxa1jsR",
      "metadata": {
        "id": "7-z_Fgxa1jsR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0%\n",
            "2%\n",
            "4%\n",
            "7%\n",
            "9%\n",
            "12%\n",
            "14%\n",
            "17%\n",
            "19%\n",
            "22%\n",
            "24%\n",
            "26%\n",
            "29%\n",
            "31%\n",
            "34%\n",
            "36%\n",
            "39%\n",
            "41%\n",
            "44%\n",
            "46%\n",
            "49%\n",
            "51%\n",
            "53%\n",
            "56%\n",
            "58%\n",
            "61%\n",
            "63%\n",
            "66%\n",
            "68%\n",
            "71%\n",
            "73%\n",
            "76%\n",
            "78%\n",
            "80%\n",
            "83%\n",
            "85%\n",
            "88%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_1327/183692495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mword_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WordFreq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muniq_wordict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mword_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Artist'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniq_wordict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'UniqueWords'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0muniq_wordict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muniq_words\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mword_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Artist'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'WordFreqList'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_1327/183692495.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mword_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WordFreq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muniq_wordict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mword_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Artist'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniq_wordict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'UniqueWords'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0muniq_wordict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muniq_words\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mword_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Artist'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'WordFreqList'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "word_df = new_df[['Genre', 'WordFreq', 'Artist', 'UniqueWords']]\n",
        "\n",
        "for index,row in word_df.iterrows():\n",
        "  if index % 100 == 0:\n",
        "    print(f'\\r{100 * index // len(new_df)}%', end='')\n",
        "  for k in row['WordFreq']:\n",
        "    # uniq_wordict[k] = row['WordFreq'][k] # original (raw count)\n",
        "    uniq_wordict[k] = row['WordFreq'][k] * math.log(len(new_df) / word_appearance_cnt[k]) # tf-idf\n",
        "\n",
        "  word_df.at[index, 'WordFreq'] = uniq_wordict\n",
        "  word_df.at[index, 'Artist'] = list(uniq_wordict.values()) + [row['UniqueWords']]\n",
        "  uniq_wordict = {k : 0 for k in uniq_words}\n",
        "\n",
        "word_df = word_df.replace({'Artist':'WordFreqList'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "qvjIQJlVh5El",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvjIQJlVh5El",
        "outputId": "95831e5d-5e69-48ab-f4a5-913dca418ae1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "109"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# free up memory\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tnv2CZeGco9t",
      "metadata": {
        "id": "Tnv2CZeGco9t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Genre                                           WordFreq  \\\n",
            "0       pop  {'might': 8.870149554808703, 'never': 3.944954...   \n",
            "1       pop  {'written': 14.83819836344378, 'walls': 7.2440...   \n",
            "2       pop  {'got': 3.1037276581915667, 'fire': 4.87686244...   \n",
            "3       pop  {'insecure': 5.1778677352052505, 'know': 9.906...   \n",
            "4       pop  {'going': 4.476031840091433, 'tonight': 7.1593...   \n",
            "...     ...                                                ...   \n",
            "4074  metal  {'sick': 3.4613316872147832, 'tired': 3.269936...   \n",
            "4075  metal  {'ready': 2.995241957290184, 'get': 1.69999781...   \n",
            "4076  metal  {'sitting': 3.5859741324220598, 'dark': 2.8121...   \n",
            "4077  metal  {'lets': 4.614017583065335, 'welcome': 8.43803...   \n",
            "4078  metal  {'saturday': 81.37902425081008, 'feel': 4.0420...   \n",
            "\n",
            "                                           WordFreqList  UniqueWords  \n",
            "0     [8.870149554808703, 3.9449546355004306, 6.3674...           71  \n",
            "1     [14.83819836344378, 7.244028137810513, 4.87937...           68  \n",
            "2     [3.1037276581915667, 4.87686244056474, 3.17665...           37  \n",
            "3     [5.1778677352052505, 9.90613155808589, 3.94391...           54  \n",
            "4     [4.476031840091433, 7.1593077754919685, 36.605...           65  \n",
            "...                                                 ...          ...  \n",
            "4074  [3.4613316872147832, 3.269936834215154, 2.6331...           61  \n",
            "4075  [2.995241957290184, 1.6999978112287593, 2.9427...           74  \n",
            "4076  [3.5859741324220598, 2.8121037405896736, 2.700...           41  \n",
            "4077  [4.614017583065335, 8.4380347778246, 3.7899940...           65  \n",
            "4078  [81.37902425081008, 4.042013292084863, 1.21581...           45  \n",
            "\n",
            "[4078 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "word_df = word_df.rename(columns = {'Artist':'WordFreqList'})\n",
        "print(word_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MrdPGQ_F85Qy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrdPGQ_F85Qy",
        "outputId": "6670b445-cbbb-4e60-a293-3e9a6d9a37d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split ratio: 0.20009808729769496\n",
            "Test set: 816\n",
            "Train set: 3262\n"
          ]
        }
      ],
      "source": [
        "# make a 80/20 train/test split of lyric words tagged by origin genre\n",
        "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(list(word_df['WordFreqList']), list(word_df['Genre']), test_size=0.2)\n",
        "\n",
        "print(f'Split ratio: {len(X_test) / (len(X_test) + len(X_train))}\\nTest set: {len(X_test)}\\nTrain set: {len(X_train)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5gqe0QFr0l2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5gqe0QFr0l2",
        "outputId": "718a5251-4e9c-4ee1-f5f1-1bdbf4e7df5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[1][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cCqVqnRI_kje",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCqVqnRI_kje",
        "outputId": "3b7037b9-533d-4611-d3c5-fca88d832f98"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3262,) + inhomogeneous part.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_1327/2958666272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \"\"\"\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    938\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mcheck_X_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             )\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    576\u001b[0m                 \u001b[0;31m# :(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3262,) + inhomogeneous part."
          ]
        }
      ],
      "source": [
        "clf = tree.DecisionTreeClassifier()\n",
        "clf.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4nUh3qoxJjhH",
      "metadata": {
        "id": "4nUh3qoxJjhH"
      },
      "outputs": [],
      "source": [
        "preds = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ItmRqJKJ0Va",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ItmRqJKJ0Va",
        "outputId": "be359428-02b9-41b6-ada2-09b3997b384e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.39215686274509803"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sum(preds == Y_test)/len(preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GWOk4XcpwP5h",
      "metadata": {
        "id": "GWOk4XcpwP5h"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "# STOP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5CXrImqRawnX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CXrImqRawnX",
        "outputId": "9cc9c891-4090-4edc-a16e-1b45bdb625b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imported 166 lines\n",
            "Identified 7 genres: {'soul_rb', 'metal', 'alternative', 'rock', 'country', 'pop', 'rap_hiphop'}\n",
            "Extracted 7 genres\n"
          ]
        }
      ],
      "source": [
        "# I'm opening the csv of lyrics from songlyrics.com, which is structured differently than the genius one.\n",
        "# might need to restructure this code (or the genius csv) if we want to use that too\n",
        "# the only difference is all the lyrics by each artist are combined into a single cell (as opposed to split by song)\n",
        "df = pd.read_csv('data/all_sl.csv')\n",
        "print(f'Imported {len(df)} lines')\n",
        "genres = set(df['Genre'])\n",
        "print(f'Identified {len(genres)} genres: {genres}')\n",
        "lyrics = defaultdict(str)\n",
        "for genre in genres:\n",
        "    for artist in list(df[df['Genre'] == genre]['Lyrics']): # grab each row (artist) and select the lyrics cell\n",
        "        lyrics[genre] += artist # combine each batch of lyrics into the respective genre in the dictionary\n",
        "print(f'Extracted {len(lyrics)} genres')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MNK8qUKlawnY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNK8qUKlawnY",
        "outputId": "5f0af8aa-a661-4f0b-ffd8-0b97868483b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[soul_rb] Identified 4626 unique words\n",
            "[metal] Identified 8963 unique words\n",
            "[alternative] Identified 5750 unique words\n",
            "[rock] Identified 6416 unique words\n",
            "[country] Identified 6533 unique words\n",
            "[pop] Identified 7269 unique words\n",
            "[rap_hiphop] Identified 15104 unique words\n"
          ]
        }
      ],
      "source": [
        "frequency_tables = {}\n",
        "words_uniq = {}\n",
        "lines_uniq = {}\n",
        "for (genre,lyric) in lyrics.items():\n",
        "    frequency_tables[genre] = sl.words_freq(lyric, stopwords)\n",
        "    words_uniq[genre] = sl.words(lyric, stopwords)\n",
        "    print(f'[{genre}] Identified {len(words_uniq[genre])} unique words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aCzIv-Ai5zU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aCzIv-Ai5zU",
        "outputId": "b2d9b473-1d4d-4361-9620-b9d77950599d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "703734"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(lyrics['alternative'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kyxLZ9lKawnZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyxLZ9lKawnZ",
        "outputId": "e4f4939e-2648-4d8e-b07a-69844249333f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### soul_rb ###\n",
            "[('love', 1492), ('baby', 1220), ('know', 971), ('yeah', 762), ('like', 627), ('got', 547), ('want', 477), ('come', 457), ('time', 439), ('one', 435), ('wanna', 435), ('go', 424), ('let', 395), ('make', 364), ('get', 350), ('say', 346), ('right', 310), ('way', 309), ('see', 307), ('girl', 306), ('cause', 300), ('day', 296), ('aint', 274), ('never', 273), ('good', 264)]\n",
            "### metal ###\n",
            "[('one', 573), ('like', 565), ('never', 543), ('know', 507), ('time', 504), ('love', 494), ('see', 492), ('get', 473), ('away', 452), ('life', 449), ('take', 439), ('yeah', 434), ('let', 380), ('way', 368), ('die', 348), ('got', 348), ('feel', 343), ('go', 341), ('world', 337), ('eyes', 321), ('come', 316), ('inside', 303), ('say', 279), ('man', 274), ('want', 272)]\n",
            "### alternative ###\n",
            "[('know', 838), ('love', 678), ('time', 652), ('like', 599), ('go', 596), ('got', 543), ('one', 525), ('never', 506), ('take', 457), ('back', 447), ('away', 445), ('say', 440), ('come', 432), ('get', 418), ('want', 417), ('right', 405), ('see', 401), ('let', 376), ('yeah', 369), ('way', 369), ('cause', 368), ('na', 351), ('feel', 342), ('well', 325), ('make', 314)]\n",
            "### rock ###\n",
            "[('love', 1159), ('yeah', 869), ('know', 793), ('like', 679), ('got', 659), ('one', 628), ('away', 580), ('go', 551), ('get', 525), ('never', 525), ('time', 524), ('want', 520), ('come', 440), ('way', 432), ('well', 430), ('say', 423), ('baby', 422), ('take', 394), ('gonna', 347), ('see', 346), ('let', 333), ('give', 323), ('back', 309), ('life', 306), ('wanna', 302)]\n",
            "### country ###\n",
            "[('like', 1033), ('love', 777), ('know', 714), ('got', 642), ('baby', 539), ('one', 524), ('little', 518), ('get', 516), ('yeah', 490), ('go', 488), ('time', 465), ('back', 452), ('aint', 434), ('never', 427), ('see', 418), ('gonna', 413), ('take', 396), ('well', 383), ('could', 376), ('night', 376), ('way', 367), ('cause', 365), ('man', 364), ('old', 354), ('away', 352)]\n",
            "### pop ###\n",
            "[('love', 2008), ('know', 1645), ('like', 1463), ('baby', 1121), ('got', 1087), ('yeah', 1053), ('go', 1007), ('get', 967), ('one', 868), ('cause', 867), ('let', 806), ('girl', 796), ('say', 727), ('make', 723), ('la', 709), ('want', 688), ('wanna', 667), ('time', 656), ('never', 643), ('need', 611), ('take', 606), ('right', 605), ('way', 580), ('heart', 573), ('see', 566)]\n",
            "### rap_hiphop ###\n",
            "[('like', 2874), ('got', 2079), ('get', 1991), ('nigga', 1790), ('know', 1750), ('aint', 1316), ('yeah', 1286), ('niggas', 1236), ('bitch', 1206), ('shit', 1184), ('fuck', 1143), ('love', 943), ('back', 891), ('see', 860), ('money', 799), ('go', 784), ('cause', 770), ('baby', 708), ('thats', 705), ('want', 702), ('man', 674), ('make', 669), ('em', 668), ('say', 660), ('right', 591)]\n"
          ]
        }
      ],
      "source": [
        "# as an example, print the top 25 most common terms from each genre!\n",
        "for genre in frequency_tables:\n",
        "    print(f'### {genre} ###\\n{sorted(frequency_tables[genre].items(), key=lambda x: x[1], reverse=True)[:25]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ml5XQ3uSawnZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "Ml5XQ3uSawnZ",
        "outputId": "4e8e4e75-23f3-436e-807a-7501608322f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split size: 0.20001463566345293\n",
            "Test set: 10933\n",
            "Train set: 43728\n"
          ]
        }
      ],
      "source": [
        "# make a 80/20 train/test split of lyric words tagged by origin genre\n",
        "X = []\n",
        "Y = []\n",
        "for (genre,words) in words_uniq.items():\n",
        "    X += words\n",
        "    Y += [genre] * len(words)\n",
        "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X, Y, test_size=0.2)\n",
        "print(f'Split size: {len(X_test) / (len(X_test) + len(X_train))}\\nTest set: {len(X_test)}\\nTrain set: {len(X_train)}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "filtered.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
