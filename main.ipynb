{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GenreMatch\n",
    "Data cleaning and model generation  \n",
    "*Jeremy Freedman, Reza Madhavan, Kunal Sheth*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils.songlyrics as sl\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk import download as nltk_download\n",
    "import sklearn as sk"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# download the NLTK stopwords list, if necessary\n",
    "nltk_download('stopwords')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jerem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# we need to clean and update the NLTK stopwords for our data\n",
    "# we're stripping out punctuation entirely, which the stopwords are not equipped to handle\n",
    "# could optionally add music-centric stopwords ('oh', 'yeah', 'like', etc)\n",
    "\n",
    "temp_words = sw.words('english')\n",
    "stopwords = []\n",
    "additions = ['im', 'ill', 'id', 'oh', 'cant', 'ive']\n",
    "for w in temp_words:\n",
    "    stopwords.append(sl._clean(w))\n",
    "stopwords += additions\n",
    "print(stopwords)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'dont', 'should', 'shouldve', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', 'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 'isn', 'isnt', 'ma', 'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', 'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt', 'im', 'ill', 'id', 'oh', 'cant', 'ive']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# I'm opening the csv of lyrics from songlyrics.com, which is structured differently than the genius one.\n",
    "# might need to restructure this code (or the genius csv) if we want to use that too\n",
    "# the only difference is all the lyrics by each artist are combined into a single cell (as opposed to split by song)\n",
    "df = pd.read_csv('data/all_sl.csv')\n",
    "print(f'Imported {len(df)} lines')\n",
    "genres = set(df['Genre'])\n",
    "print(f'Identified {len(genres)} genres: {genres}')\n",
    "lyrics = defaultdict(str)\n",
    "for genre in genres:\n",
    "    for artist in list(df[df['Genre'] == genre]['Lyrics']): # grab each row (artist) and select the lyrics cell\n",
    "        lyrics[genre] += artist # combine each batch of lyrics into the respective genre in the dictionary\n",
    "print(f'Extracted {len(lyrics)} genres')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Imported 166 lines\n",
      "Identified 7 genres: {'metal', 'alternative', 'rap_hiphop', 'soul_rb', 'country', 'pop', 'rock'}\n",
      "Extracted 7 genres\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "frequency_tables = {}\n",
    "words_uniq = {}\n",
    "lines_uniq = {}\n",
    "for (genre,lyric) in lyrics.items():\n",
    "    frequency_tables[genre] = sl.words_freq(lyric, stopwords)\n",
    "    words_uniq[genre] = sl.words(lyric, stopwords)\n",
    "    print(f'[{genre}] Identified {len(words_uniq[genre])} unique words')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[metal] Identified 8963 unique words\n",
      "[alternative] Identified 5750 unique words\n",
      "[rap_hiphop] Identified 15104 unique words\n",
      "[soul_rb] Identified 4626 unique words\n",
      "[country] Identified 6533 unique words\n",
      "[pop] Identified 7269 unique words\n",
      "[rock] Identified 6416 unique words\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# as an example, print the top 25 most common terms from each genre!\n",
    "for genre in frequency_tables:\n",
    "    print(f'### {genre} ###\\n{sorted(frequency_tables[genre].items(), key=lambda x: x[1], reverse=True)[:25]}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### metal ###\n",
      "[('one', 573), ('like', 565), ('never', 543), ('know', 507), ('time', 504), ('love', 494), ('see', 492), ('get', 473), ('away', 452), ('life', 449), ('take', 439), ('yeah', 434), ('let', 380), ('way', 368), ('die', 348), ('got', 348), ('feel', 343), ('go', 341), ('world', 337), ('eyes', 321), ('come', 316), ('inside', 303), ('say', 279), ('man', 274), ('want', 272)]\n",
      "### alternative ###\n",
      "[('know', 838), ('love', 678), ('time', 652), ('like', 599), ('go', 596), ('got', 543), ('one', 525), ('never', 506), ('take', 457), ('back', 447), ('away', 445), ('say', 440), ('come', 432), ('get', 418), ('want', 417), ('right', 405), ('see', 401), ('let', 376), ('yeah', 369), ('way', 369), ('cause', 368), ('na', 351), ('feel', 342), ('well', 325), ('make', 314)]\n",
      "### rap_hiphop ###\n",
      "[('like', 2874), ('got', 2079), ('get', 1991), ('nigga', 1790), ('know', 1750), ('aint', 1316), ('yeah', 1286), ('niggas', 1236), ('bitch', 1206), ('shit', 1184), ('fuck', 1143), ('love', 943), ('back', 891), ('see', 860), ('money', 799), ('go', 784), ('cause', 770), ('baby', 708), ('thats', 705), ('want', 702), ('man', 674), ('make', 669), ('em', 668), ('say', 660), ('right', 591)]\n",
      "### soul_rb ###\n",
      "[('love', 1492), ('baby', 1220), ('know', 971), ('yeah', 762), ('like', 627), ('got', 547), ('want', 477), ('come', 457), ('time', 439), ('one', 435), ('wanna', 435), ('go', 424), ('let', 395), ('make', 364), ('get', 350), ('say', 346), ('right', 310), ('way', 309), ('see', 307), ('girl', 306), ('cause', 300), ('day', 296), ('aint', 274), ('never', 273), ('good', 264)]\n",
      "### country ###\n",
      "[('like', 1033), ('love', 777), ('know', 714), ('got', 642), ('baby', 539), ('one', 524), ('little', 518), ('get', 516), ('yeah', 490), ('go', 488), ('time', 465), ('back', 452), ('aint', 434), ('never', 427), ('see', 418), ('gonna', 413), ('take', 396), ('well', 383), ('could', 376), ('night', 376), ('way', 367), ('cause', 365), ('man', 364), ('old', 354), ('away', 352)]\n",
      "### pop ###\n",
      "[('love', 2008), ('know', 1645), ('like', 1463), ('baby', 1121), ('got', 1087), ('yeah', 1053), ('go', 1007), ('get', 967), ('one', 868), ('cause', 867), ('let', 806), ('girl', 796), ('say', 727), ('make', 723), ('la', 709), ('want', 688), ('wanna', 667), ('time', 656), ('never', 643), ('need', 611), ('take', 606), ('right', 605), ('way', 580), ('heart', 573), ('see', 566)]\n",
      "### rock ###\n",
      "[('love', 1159), ('yeah', 869), ('know', 793), ('like', 679), ('got', 659), ('one', 628), ('away', 580), ('go', 551), ('get', 525), ('never', 525), ('time', 524), ('want', 520), ('come', 440), ('way', 432), ('well', 430), ('say', 423), ('baby', 422), ('take', 394), ('gonna', 347), ('see', 346), ('let', 333), ('give', 323), ('back', 309), ('life', 306), ('wanna', 302)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# make a 80/20 train/test split of lyric words tagged by origin genre\n",
    "X = []\n",
    "Y = []\n",
    "for (genre,words) in words_uniq.items():\n",
    "    X += words\n",
    "    Y += [genre] * len(words)\n",
    "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X, Y, test_size=0.2)\n",
    "print(f'Split size: {len(X_test) / (len(X_test) + len(X_train))}\\nTest set: {len(X_test)}\\nTrain set: {len(X_train)}')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Split size: 0.20001463566345293\n",
      "Test set: 10933\n",
      "Train set: 43728\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}